{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fahrizaydr/Epub-Arabic-TTS-Simulation/blob/main/Epub_to_Audio_Facebook_nllb_200_distilled_600M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4O4GoHZ42J"
      },
      "source": [
        "Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3qxkRDZwMpV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets ebooklib beautifulsoup4 huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq4eN61yZ_Hr"
      },
      "source": [
        "Upload Epub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvrbpT0Txz_I"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload file EPUB\n",
        "uploaded = files.upload()\n",
        "\n",
        "epub_path = next(iter(uploaded))\n",
        "print(f\"File EPUB yang diupload: {epub_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bqRb0TOaCDQ"
      },
      "source": [
        "Baca Epub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OczSUAPSz5Qh"
      },
      "outputs": [],
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def extract_text_from_epub(file_path):\n",
        "    book = epub.read_epub(file_path)\n",
        "    texts = []\n",
        "\n",
        "    for item in book.get_items():\n",
        "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
        "            content = item.get_content()\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "            texts.append(soup.get_text())\n",
        "\n",
        "    full_text = \"\\n\".join(texts)\n",
        "    return full_text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7Fء-ي]+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Ekstrak dan bersihkan teks\n",
        "raw_text = extract_text_from_epub(epub_path)\n",
        "arabic_text = clean_text(raw_text)\n",
        "\n",
        "# Preview teks\n",
        "print(\"Preview teks Arab:\")\n",
        "print(arabic_text[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmuPiVPUaOCB"
      },
      "source": [
        "Token Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZTO_PhyaNOO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "import getpass\n",
        "\n",
        "token = getpass.getpass(\"Masukkan Hugging Face token kamu: \")\n",
        "login(token)\n",
        "os.environ[\"HF_TOKEN\"] = token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tobt0ndDaFLN"
      },
      "source": [
        "Membagi teks pada epub dengan titik sebagai pembatas antar kalimat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNHVBUzC7BNY"
      },
      "outputs": [],
      "source": [
        "def split_text_by_period(text):\n",
        "    sentences = text.split('.')\n",
        "    chunks = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        clean_sentence = sentence.strip()\n",
        "        if clean_sentence:\n",
        "            chunks.append(clean_sentence + '.')\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Proses split\n",
        "chunks = split_text_by_period(arabic_text)\n",
        "\n",
        "print(f\"Total potongan teks (chunk): {len(chunks)}\")\n",
        "\n",
        "try:\n",
        "    limit = int(input(f\"Ingin gunakan berapa chunk pertama? (max {len(chunks)}): \"))\n",
        "    limit = min(limit, len(chunks))\n",
        "except:\n",
        "    print(\"Input tidak valid, gunakan semua chunk.\")\n",
        "    limit = len(chunks)\n",
        "\n",
        "selected_chunks = chunks[:limit]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUozqJrDkRTK"
      },
      "source": [
        "Translate Arab > Inggris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R6J3tScZ8Rd"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load model NLLB\n",
        "translator = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\")\n",
        "\n",
        "def translate_text(text):\n",
        "    return translator(text, src_lang=\"arb\", tgt_lang=\"eng_Latn\")[0]['translation_text']\n",
        "\n",
        "# Terjemahkan\n",
        "translated_chunks = []\n",
        "for idx, chunk in enumerate(selected_chunks):\n",
        "    print(f\"Translating chunk {idx+1}/{len(selected_chunks)}\")\n",
        "    try:\n",
        "        translated = translate_text(chunk)\n",
        "        translated_chunks.append(translated)\n",
        "    except Exception as e:\n",
        "        print(f\" Error on chunk {idx+1}: {e}\")\n",
        "        translated_chunks.append(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gabungkan hasil translate antar chunk (Inggris)"
      ],
      "metadata": {
        "id": "2KpMMj2RO9RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi menggabungkan chunk hasil terjemahan\n",
        "def combine_translations(translated_chunks):\n",
        "    return \"\\n\\n\".join(translated_chunks)\n",
        "\n",
        "full_translation = combine_translations(translated_chunks)"
      ],
      "metadata": {
        "id": "qvFRliVaRGMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ER7W-Hj1vw"
      },
      "source": [
        "Translate Arab > Indo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jllm5rNVYtPK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load model NLLB\n",
        "translator = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\")\n",
        "\n",
        "def translate_text(text):\n",
        "    return translator(text, src_lang=\"arb\", tgt_lang=\"ind_Latn\")[0]['translation_text']\n",
        "\n",
        "# Terjemahkan\n",
        "translated_chunks = []\n",
        "for idx, chunk in enumerate(selected_chunks):\n",
        "    print(f\"Translating chunk {idx+1}/{len(selected_chunks)}\")\n",
        "    try:\n",
        "        translated = translate_text(chunk)\n",
        "        translated_chunks.append(translated)\n",
        "    except Exception as e:\n",
        "        print(f\" Error on chunk {idx+1}: {e}\")\n",
        "        translated_chunks.append(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gabungkan hasil translate antar chunk (Indo)"
      ],
      "metadata": {
        "id": "G9ob99-_RK0O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqobVSsjZ_EX"
      },
      "outputs": [],
      "source": [
        "# Fungsi menggabungkan chunk hasil terjemahan\n",
        "def combine_translations(translated_chunks):\n",
        "    return \"\\n\\n\".join(translated_chunks)\n",
        "\n",
        "full_translation = combine_translations(translated_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download TXT"
      ],
      "metadata": {
        "id": "tI-9ddv7PD2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan ke file txt\n",
        "output_file = \"hasil_terjemahan.txt\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(full_translation)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "\n",
        "print(f\"File hasil terjemahan berhasil disimpan: {output_file}\")"
      ],
      "metadata": {
        "id": "1j3586WXOuxx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOOyDoswAiwJ1aLTe4iW9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}